{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65503516",
   "metadata": {},
   "source": [
    "\n",
    "# PLP Frontend (Colab Edition): Gradio/Streamlit UI for the Corporate-Finance Backend\n",
    "\n",
    "Works with your Colab backend corpus at `My Drive/Project_3/data/corpus_docs.jsonl`.\n",
    "\n",
    "- **Gradio UI** (in-notebook)\n",
    "- **Streamlit app** (optional, via ngrok)\n",
    "\n",
    "Shared backend API: `init_backend(corpus_jsonl_path)`, `answer(query, k=4, show_steps=True)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb3fac",
   "metadata": {},
   "source": [
    "## Step 0 â€” Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0786912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install scikit-learn gradio streamlit pyngrok==4.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79648c27",
   "metadata": {},
   "source": [
    "## Step 1 â€” Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c530ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0aa2",
   "metadata": {},
   "source": [
    "## Step 2 â€” Write Shared Backend Module (`plp_backend_colab.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code = r'''\n",
    "# plp_backend_colab.py\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json, re, textwrap\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    title: str\n",
    "    url: str\n",
    "    text: str\n",
    "\n",
    "_chunks = None\n",
    "_index = None\n",
    "\n",
    "def _simple_sentence_split(text: str):\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "def _build_chunks(docs: List[Dict[str,str]], max_sent_per_chunk: int = 2) -> List[Chunk]:\n",
    "    chunks: List[Chunk] = []\n",
    "    for d in docs:\n",
    "        doc_id = d[\"id\"]\n",
    "        title = d.get(\"title\",\"\")\n",
    "        url = d.get(\"url\",\"\")\n",
    "        text = d.get(\"text\",\"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        sents = _simple_sentence_split(text)\n",
    "        for i in range(0, len(sents), max_sent_per_chunk):\n",
    "            piece = \" \".join(sents[i:i+max_sent_per_chunk])\n",
    "            chunks.append(Chunk(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=f\"{doc_id}::ch{i//max_sent_per_chunk}\",\n",
    "                title=title,\n",
    "                url=url,\n",
    "                text=piece\n",
    "            ))\n",
    "    return chunks\n",
    "\n",
    "class _TfidfIndex:\n",
    "    def __init__(self, chunks: List[Chunk]):\n",
    "        self.chunks = chunks\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "        self.matrix = self.vectorizer.fit_transform([c.text for c in chunks])\n",
    "    def query(self, q: str, top_k: int = 5) -> List[Tuple[Chunk, float]]:\n",
    "        qv = self.vectorizer.transform([q])\n",
    "        sims = cosine_similarity(qv, self.matrix)[0]\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        return [(self.chunks[i], float(sims[i])) for i in idx]\n",
    "\n",
    "def _cot_plan(query: str):\n",
    "    return [\n",
    "        \"1) Identify the finance concept(s) asked.\",\n",
    "        \"2) Retrieve definitions/formulas and usage conditions.\",\n",
    "        \"3) Cross-check top chunks for consistency and specificity.\",\n",
    "        \"4) Compose a concise, grounded answer with citations.\"\n",
    "    ]\n",
    "\n",
    "def _synthesize_answer(query: str, retrieved, show_steps=True):\n",
    "    bullets = []\n",
    "    for ch, score in retrieved:\n",
    "        snippet = textwrap.shorten(ch.text, width=220, placeholder=\"â€¦\")\n",
    "        bullets.append(f\"- [{ch.doc_id}] {snippet} (score={score:.3f})\")\n",
    "    ql = query.lower()\n",
    "    lines = []\n",
    "    if \"wacc\" in ql or \"cost of capital\" in ql:\n",
    "        lines.append(\"WACC = E/VÂ·Re + D/VÂ·RdÂ·(1âˆ’Tc). Use it when project risk is similar to the firmâ€™s core assets.\")\n",
    "    if \"npv\" in ql or \"irr\" in ql:\n",
    "        lines.append(\"NPV = Î£ CFt/(1+r)^t âˆ’ initial cost; accept if NPV>0. IRR sets NPV=0; best for conventional cash flows and comparable scales.\")\n",
    "    if not lines:\n",
    "        lines.append(textwrap.shorten(\" \".join([ch.text for ch,_ in retrieved]), width=500, placeholder=\"â€¦\"))\n",
    "    citations = [{\"doc_id\": ch.doc_id, \"chunk_id\": ch.chunk_id, \"url\": ch.url} for ch,_ in retrieved]\n",
    "    out = {\"query\": query, \"answer\": \" \".join(lines), \"citations\": citations}\n",
    "    if show_steps:\n",
    "        out[\"reasoning_steps\"] = _cot_plan(query) + bullets\n",
    "    return out\n",
    "\n",
    "def init_backend(corpus_jsonl_path: str, max_sent_per_chunk: int = 2) -> int:\n",
    "    global _chunks, _index\n",
    "    docs: List[Dict[str,str]] = []\n",
    "    p = Path(corpus_jsonl_path)\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            docs.append({\n",
    "                \"id\": rec[\"id\"],\n",
    "                \"title\": rec.get(\"title\",\"\"),\n",
    "                \"url\": rec.get(\"url\",\"\"),\n",
    "                \"text\": rec.get(\"text\",\"\")\n",
    "            })\n",
    "    _chunks = _build_chunks(docs, max_sent_per_chunk=max_sent_per_chunk)\n",
    "    _index = _TfidfIndex(_chunks)\n",
    "    return len(_chunks)\n",
    "\n",
    "def answer(query: str, k: int = 4, show_steps: bool=True):\n",
    "    assert _index is not None, \"Backend not initialized. Call init_backend().\"\n",
    "    retrieved = _index.query(query, top_k=k)\n",
    "    return _synthesize_answer(query, retrieved, show_steps=show_steps)\n",
    "'''\n",
    "open('/content/plp_backend_colab.py','w').write(code)\n",
    "print(\"Wrote /content/plp_backend_colab.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6f698",
   "metadata": {},
   "source": [
    "## Step 3 â€” Initialize Backend with Drive Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plp_backend_colab import init_backend, answer\n",
    "CORPUS_JSONL = '/content/drive/My Drive/Project_3/data/corpus_docs.jsonl'  # change if needed\n",
    "n_chunks = init_backend(CORPUS_JSONL, max_sent_per_chunk=2)\n",
    "print(\"Backend ready. Chunks:\", n_chunks)\n",
    "print(answer(\"What is WACC?\", show_steps=False)[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548dc5db",
   "metadata": {},
   "source": [
    "## Step 4 â€” Gradio UI (in-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, gradio as gr\n",
    "from plp_backend_colab import answer\n",
    "\n",
    "def gradio_answer(q, k, show):\n",
    "    if not q or not q.strip():\n",
    "        return \"Please enter a question.\", \"\"\n",
    "    resp = answer(q, k=int(k), show_steps=bool(show))\n",
    "    cites = json.dumps(resp[\"citations\"], indent=2)\n",
    "    reasoning = \"\\n\".join(resp.get(\"reasoning_steps\", []))\n",
    "    out = resp[\"answer\"]\n",
    "    if show and reasoning:\n",
    "        out += \"\\n\\n-- Visible Reasoning --\\n\" + reasoning\n",
    "    return out, cites\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ“˜ PLP â€“ Corporate Finance (Gradio)\")\n",
    "    q = gr.Textbox(label=\"Question\", placeholder=\"How do I compute WACC and when to use it?\")\n",
    "    k = gr.Slider(2, 8, value=4, step=1, label=\"Top-k\")\n",
    "    show = gr.Checkbox(label=\"Show Reasoning (CoT-style)\", value=True)\n",
    "    btn = gr.Button(\"Ask\")\n",
    "    ans = gr.Textbox(label=\"Answer\", lines=8)\n",
    "    cites = gr.Textbox(label=\"Citations (JSON)\", lines=8)\n",
    "    btn.click(fn=gradio_answer, inputs=[q,k,show], outputs=[ans,cites])\n",
    "\n",
    "# Uncomment to launch:\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6c07b",
   "metadata": {},
   "source": [
    "## Step 5 â€” Optional: Streamlit + ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write Streamlit app file\n",
    "app = r'''\n",
    "import streamlit as st, json\n",
    "from plp_backend_colab import answer\n",
    "\n",
    "st.set_page_config(page_title=\"PLP â€“ Corporate Finance\", page_icon=\"ðŸ“˜\", layout=\"wide\")\n",
    "st.title(\"ðŸ“˜ PLP â€“ Corporate Finance (Streamlit)\")\n",
    "\n",
    "k = st.sidebar.slider(\"Top-k Chunks\", 2, 8, 4, 1)\n",
    "show = st.sidebar.checkbox(\"Show Reasoning (CoT-style)\", value=True)\n",
    "\n",
    "q = st.text_input(\"Enter your question:\", placeholder=\"e.g., Compare NPV and IRR\")\n",
    "if st.button(\"Ask\"):\n",
    "    if not q.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        resp = answer(q, k=k, show_steps=show)\n",
    "        st.subheader(\"Answer\")\n",
    "        st.write(resp[\"answer\"])\n",
    "        st.subheader(\"Citations\")\n",
    "        st.json(resp[\"citations\"])\n",
    "        if show and \"reasoning_steps\" in resp:\n",
    "            st.subheader(\"Reasoning (Visible)\")\n",
    "            for step in resp[\"reasoning_steps\"]:\n",
    "                st.markdown(f\"- {step}\")\n",
    "'''\n",
    "open('/content/plp_streamlit_app_colab.py','w').write(app)\n",
    "print(\"Wrote /content/plp_streamlit_app_colab.py\")\n",
    "\n",
    "# Optional: start Streamlit via ngrok (uncomment to run)\n",
    "# from pyngrok import ngrok\n",
    "# public_url = ngrok.connect(8501).public_url\n",
    "# print(\"Public URL:\", public_url)\n",
    "# !streamlit run /content/plp_streamlit_app_colab.py --server.port 8501 --server.address 0.0.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbca7a",
   "metadata": {},
   "source": [
    "\n",
    "## How to Link Frontend & Backend\n",
    "\n",
    "1. Run **Steps 0â€“3** to install deps, mount Drive, and initialize the backend with your JSONL corpus.\n",
    "2. **Gradio** (Step 4): uncomment `demo.launch()` and run to get a local URL.\n",
    "3. **Streamlit** (Step 5): write the app file; optionally start an ngrok tunnel and run Streamlit.\n",
    "\n",
    "Both UIs import `answer()` from `plp_backend_colab.py`. You can swap TFâ€“IDF for embeddings or plug in an LLM\n",
    "later without changing UI code, as long as `answer()` keeps returning `{answer, citations, reasoning_steps?}`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}