{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ced8b5",
   "metadata": {},
   "source": [
    "\n",
    "# PLP Backend (Colab Edition): Load `corpus_docs.jsonl` from Google Drive and Query\n",
    "\n",
    "This notebook is **Colab-ready**. It:\n",
    "- Mounts Google Drive and loads your curated corpus at:  \n",
    "  `My Drive/Project_3/data/corpus_docs.jsonl`\n",
    "- Builds a lightweight **TF–IDF → cosine** retrieval index\n",
    "- Exposes a clean `answer(query, k=4, show_steps=True)` API with citations and optional CoT-style visible reasoning\n",
    "- Includes quick smoke tests and a small interactive loop\n",
    "\n",
    "> Edit the path in **Step 2** if your folder name differs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8052eeb",
   "metadata": {},
   "source": [
    "## Step 0 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172b32c",
   "metadata": {},
   "source": [
    "## Step 1 — Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c884d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 — Configure Paths\n",
    "\n",
    "Default target:\n",
    "```\n",
    "My Drive/Project_3/data/corpus_docs.jsonl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "CORPUS_JSONL = Path('/content/drive/My Drive/Project_3/data/corpus_docs.jsonl')\n",
    "print(\"Using corpus file:\", CORPUS_JSONL)\n",
    "assert CORPUS_JSONL.exists(), f\"Corpus file not found at: {CORPUS_JSONL}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd62030",
   "metadata": {},
   "source": [
    "## Step 3 — Backend: Data Types, Index, and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358eac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, re, textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    title: str\n",
    "    url: str\n",
    "    text: str\n",
    "\n",
    "def simple_sentence_split(text: str):\n",
    "    import re\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "def build_chunks(docs: List[Dict[str, str]], max_sent_per_chunk: int = 2) -> List[Chunk]:\n",
    "    chunks: List[Chunk] = []\n",
    "    for d in docs:\n",
    "        doc_id = d[\"id\"]\n",
    "        title = d.get(\"title\",\"\")\n",
    "        url = d.get(\"url\",\"\")\n",
    "        text = d.get(\"text\",\"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        sents = simple_sentence_split(text)\n",
    "        for i in range(0, len(sents), max_sent_per_chunk):\n",
    "            piece = \" \".join(sents[i:i+max_sent_per_chunk])\n",
    "            chunks.append(Chunk(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=f\"{doc_id}::ch{i//max_sent_per_chunk}\",\n",
    "                title=title,\n",
    "                url=url,\n",
    "                text=piece\n",
    "            ))\n",
    "    return chunks\n",
    "\n",
    "class TfidfIndex:\n",
    "    def __init__(self, chunks: List[Chunk]):\n",
    "        self.chunks = chunks\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "        self.matrix = self.vectorizer.fit_transform([c.text for c in chunks])\n",
    "    def query(self, q: str, top_k: int = 5) -> List[Tuple[Chunk, float]]:\n",
    "        qv = self.vectorizer.transform([q])\n",
    "        sims = cosine_similarity(qv, self.matrix)[0]\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        return [(self.chunks[i], float(sims[i])) for i in idx]\n",
    "\n",
    "def cot_plan(query: str):\n",
    "    return [\n",
    "        \"1) Identify the finance concept(s) asked.\",\n",
    "        \"2) Retrieve definitions/formulas and usage conditions.\",\n",
    "        \"3) Cross-check top chunks for consistency and specificity.\",\n",
    "        \"4) Compose a concise, grounded answer with citations.\"\n",
    "    ]\n",
    "\n",
    "def synthesize_answer(query: str, retrieved, show_steps=True):\n",
    "    bullets = []\n",
    "    for ch, score in retrieved:\n",
    "        import textwrap\n",
    "        snippet = textwrap.shorten(ch.text, width=220, placeholder=\"…\")\n",
    "        bullets.append(f\"- [{ch.doc_id}] {snippet} (score={score:.3f})\")\n",
    "    ql = query.lower()\n",
    "    lines = []\n",
    "    if \"wacc\" in ql or \"cost of capital\" in ql:\n",
    "        lines.append(\"WACC = E/V·Re + D/V·Rd·(1−Tc). Use it when project risk is similar to the firm’s core assets.\")\n",
    "    if \"npv\" in ql or \"irr\" in ql:\n",
    "        lines.append(\"NPV = Σ CFt/(1+r)^t − initial cost; accept if NPV>0. IRR sets NPV=0; best for conventional cash flows and comparable scales.\")\n",
    "    if not lines:\n",
    "        lines.append(textwrap.shorten(\" \".join([ch.text for ch,_ in retrieved]), width=500, placeholder=\"…\"))\n",
    "    citations = [{\"doc_id\": ch.doc_id, \"chunk_id\": ch.chunk_id, \"url\": ch.url} for ch,_ in retrieved]\n",
    "    out = {\"query\": query, \"answer\": \" \".join(lines), \"citations\": citations}\n",
    "    if show_steps:\n",
    "        out[\"reasoning_steps\"] = cot_plan(query) + bullets\n",
    "    return out\n",
    "\n",
    "_index = None\n",
    "_chunks = None\n",
    "\n",
    "def answer(query: str, k: int = 4, show_steps: bool=True):\n",
    "    assert _index is not None, \"Index not built. Run the ingestion cell first.\"\n",
    "    retrieved = _index.query(query, top_k=k)\n",
    "    return synthesize_answer(query, retrieved, show_steps=show_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ae4b8",
   "metadata": {},
   "source": [
    "## Step 4 — Load Corpus JSONL and Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl(path: Path, limit=None) -> List[Dict[str,str]]:\n",
    "    docs = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            rec = json.loads(line)\n",
    "            docs.append({\n",
    "                \"id\": rec[\"id\"],\n",
    "                \"title\": rec.get(\"title\",\"\"),\n",
    "                \"url\": rec.get(\"url\",\"\"),\n",
    "                \"text\": rec.get(\"text\",\"\")\n",
    "            })\n",
    "    return docs\n",
    "\n",
    "docs = load_jsonl(CORPUS_JSONL)\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "_chunks = build_chunks(docs, max_sent_per_chunk=2)\n",
    "print(\"Chunks:\", len(_chunks))\n",
    "\n",
    "_index = TfidfIndex(_chunks)\n",
    "print(\"Index built.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc52c9",
   "metadata": {},
   "source": [
    "## Step 5 — Smoke Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo1 = answer(\"How do I compute WACC and when should I use it as a discount rate?\")\n",
    "demo2 = answer(\"Compare NPV and IRR and mention a limitation of payback.\", show_steps=False)\n",
    "demo1, demo2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f8cf3",
   "metadata": {},
   "source": [
    "## Step 6 — Optional Interactive Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24451537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# while True:\n",
    "#     q = input(\"Ask a finance question (or 'quit'): \").strip()\n",
    "#     if q.lower() in {\"quit\",\"exit\"}:\n",
    "#         break\n",
    "#     resp = answer(q, k=4, show_steps=True)\n",
    "#     print(\"\\nAnswer:\", resp[\"answer\"])\n",
    "#     print(\"Citations:\", json.dumps(resp[\"citations\"], indent=2))\n",
    "#     if \"reasoning_steps\" in resp:\n",
    "#         print(\"Reasoning:\")\n",
    "#         for s in resp[\"reasoning_steps\"]:\n",
    "#             print(\"-\", s)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}